{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from sklearn import preprocessing\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Spam data\n",
    "spam_train_X = sio.loadmat('/Users/aiswaryasankar/Desktop/cs189/hw5/dist/spam_data.mat')['training_data']\n",
    "spam_train_y = sio.loadmat('/Users/aiswaryasankar/Desktop/cs189/hw5/dist/spam_data.mat')['training_labels']\n",
    "spam_test = sio.loadmat('/Users/aiswaryasankar/Desktop/cs189/hw5/dist/spam_data.mat')['test_data']\n",
    "\n",
    "# Census data\n",
    "census_train_df = pd.read_csv('/Users/aiswaryasankar/Desktop/cs189/hw5/hw5_census_dist/train_data.csv')\n",
    "census_test_df = pd.read_csv('/Users/aiswaryasankar/Desktop/cs189/hw5/hw5_census_dist/test_data.csv')\n",
    "\n",
    "# Titanic data\n",
    "titanic_test_df = pd.read_csv('/Users/aiswaryasankar/Desktop/cs189/hw5/hw5_titanic_dist/titanic_testing_data.csv')\n",
    "titanic_train_df = pd.read_csv('/Users/aiswaryasankar/Desktop/cs189/hw5/hw5_titanic_dist/titanic_training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Titanic Data Preprocessing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Impute missing values, use fillna\n",
    "\n",
    "def fill_null(var):\n",
    "    titanic_train_df[var] = titanic_train_df[var].fillna(titanic_train_df[var].median())\n",
    "    \n",
    "def titanic_preprocess(data):\n",
    "    # Pass in each of the numerical columns and fill with the median\n",
    "    vals = ['pclass', 'age', 'sibsp', 'parch', 'fare']\n",
    "    for val in vals:\n",
    "        fill_null(val)\n",
    "    \n",
    "    # Note that the embarked column also has missing values\n",
    "    # titanic_train_df['embarked'] = titanic_train_df['embarked'].fillna('S')\n",
    "    data['embarked'] = data['embarked'].fillna('S')\n",
    "    \n",
    "    del data['cabin']\n",
    "    del data['ticket']\n",
    "    data = pd.get_dummies(data)\n",
    "    return data\n",
    "    \n",
    "titanic_train_df = titanic_preprocess(titanic_train_df)\n",
    "\n",
    "# Split the data into data and labels\n",
    "titanic_train_labels = titanic_train_df['survived']\n",
    "titanic_train_df = titanic_train_df.iloc[:, 1:]\n",
    "\n",
    "titanic_test_df = titanic_preprocess(titanic_test_df)\n",
    "\n",
    "# Train_test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "titanic_train_df, titanic_val_df, titanic_train_labels, titanic_val_labels = train_test_split(titanic_train_df, titanic_train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Census Data Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32724, 65)\n",
      "(32724,)\n"
     ]
    }
   ],
   "source": [
    "# All the variables have the same count but there are ? instead\n",
    "# Thus first go through and change the ? into NA\n",
    "def census_preprocess(data):\n",
    "    data = data.replace('?', np.nan)\n",
    "\n",
    "    # Convert all the categorical variables into binary\n",
    "    # I'm going to remove native_country before doing this because too many native countries\n",
    "    del data['native-country']\n",
    "    data = pd.get_dummies(data)\n",
    "    return data\n",
    "    \n",
    "census_train_df = census_preprocess(census_train_df)\n",
    "census_train_labels = census_train_df['label']\n",
    "print(census_train_df.shape)\n",
    "print(census_train_labels.shape)\n",
    "del census_train_df['label']\n",
    "\n",
    "census_test_df = census_preprocess(census_test_df)\n",
    "\n",
    "# Train test split\n",
    "census_train_df, census_val_df, census_train_labels, census_val_labels = train_test_split(census_train_df, census_train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Observation native-country_Holand-Netherlands not in the test columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spam Data Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ..., 0 0 0]\n",
      "(23702, 32)\n",
      "(23702,)\n"
     ]
    }
   ],
   "source": [
    "# No real preprocessing needed here?\n",
    "# Convert the matrices into DataFrames\n",
    "\n",
    "spam_train_df = pd.DataFrame(spam_train_X)\n",
    "spam_test_df = pd.DataFrame(spam_test\n",
    "                           )\n",
    "# spam_train_labels = pd.DataFrame(spam_train_y.T)\n",
    "print(spam_train_y[0])\n",
    "spam_train_labels = pd.Series(spam_train_y[0])\n",
    "\n",
    "print(spam_train_df.shape)\n",
    "print(spam_train_labels.shape)\n",
    "spam_train_df, spam_val_df, spam_train_labels, spam_val_labels = train_test_split(spam_train_df, \n",
    "                                                                                  spam_train_labels, \n",
    "                                                                                  test_size=0.2, \n",
    "                                                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision Tree</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections \n",
    "import timeit\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, feature, value, left, right, label):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.label = label\n",
    "        \n",
    "        \n",
    "class DecisionTree():\n",
    "    \n",
    "    def __init__(self, train_data, train_labels, val_data, val_labels):\n",
    "        self.root = Node(None, None, None, None, None)\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.train_labels = train_labels\n",
    "        self.val_labels = val_labels\n",
    "        self.numPoints = 0\n",
    "        \n",
    "    def entropy(self, numNeg, numPos):\n",
    "        if numNeg == 0 or numPos == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            total = numNeg + numPos\n",
    "            return -(numPos/total) * math.log(numPos/total, 2) - (numNeg/total) * math.log(numNeg/total, 2)\n",
    "        \n",
    "    def weighted_entropy(self, left_label_hist, right_label_hist):\n",
    "        # This method needs to compute -(p/n+p)log(p/n+p) - (n/n+p)log(n/n+p)\n",
    "        numLeftNeg = left_label_hist[0]\n",
    "        numLeftPos = left_label_hist[1]\n",
    "        numRightNeg = right_label_hist[0]\n",
    "        numRightPos = right_label_hist[1]\n",
    "        numLeft = numLeftNeg + numLeftPos\n",
    "        numRight = numRightNeg + numRightPos\n",
    "        total = numLeft + numRight\n",
    "        \n",
    "        leftEntropy = self.entropy(numLeftNeg, numLeftPos)\n",
    "        rightEntropy = self.entropy(numRightNeg, numRightPos)\n",
    "        weightedEntropy = (numLeft / total) * leftEntropy + (numRight / total) * rightEntropy\n",
    "        return weightedEntropy\n",
    "    \n",
    "    def segmenter(self, data, labels):\n",
    "        data, labels = (list(x) for x in zip(*sorted(zip(data, labels), key=lambda pair: pair[0])))\n",
    "        rightNeg = labels.count(0.0)\n",
    "        rightPos = labels.count(1.0)\n",
    "        leftNeg, leftPos = 0, 0\n",
    "        \n",
    "        dictionary = collections.OrderedDict()\n",
    "        for key, value in zip(data, labels):\n",
    "            dictionary.setdefault(key, []).append(value)\n",
    "\n",
    "        min_entropy = float('inf')\n",
    "        best_split = -1.0\n",
    "        \n",
    "        for key, value in dictionary.items():  \n",
    "            # Increment the left by the occurences of each kind and decrememnt from the right\n",
    "            leftNeg += value.count(0.0)\n",
    "            leftPos += value.count(1.0)\n",
    "            rightNeg -= value.count(0.0)\n",
    "            rightPos -= value.count(1.0)\n",
    "\n",
    "            key_entropy = self.weighted_entropy((leftNeg, leftPos), (rightNeg, rightPos))\n",
    "            \n",
    "            if key_entropy < min_entropy:\n",
    "                min_entropy = key_entropy\n",
    "                best_split = key\n",
    "                \n",
    "        return [best_split, min_entropy]\n",
    "        \n",
    "    \n",
    "    def train(self, root, data, labels, curLevel, maxLevel, percentFeatures):  \n",
    "        \n",
    "        if curLevel > maxLevel or data.empty or labels.empty:\n",
    "            labels = labels.tolist()\n",
    "            bestLabel = np.argmax([labels.count(0.0), labels.count(1.0)])\n",
    "            root.label = bestLabel\n",
    "            self.numPoints += 1\n",
    "            #print('creating a leaf with label ' + str(bestLabel) + \" at depth \" + str(curLevel))\n",
    "        else:\n",
    "            best_overall_feature = 'a'\n",
    "            best_overall_split_entropy = float('inf')\n",
    "            best_overall_split_value = float('inf')\n",
    "            \n",
    "            # This loop iterates over all of the columns and returns the best split\n",
    "            # For random forest I need to choose a random subsample of size √len(data.columns)\n",
    "            # for each split\n",
    "            # Thus I will compute this random subsample of features and iterate over that\n",
    "            # I will print out this list each time to check that I am using a random sample\n",
    "            numFeatures = int(percentFeatures * len(data.columns))\n",
    "            randCols = [data.columns[i] for i in \n",
    "                        sorted(random.sample(range(len(data.columns)), numFeatures)) ]\n",
    "            \n",
    "            start = timeit.default_timer()\n",
    "            for col in randCols:\n",
    "                \n",
    "                value, best_split_entropy = self.segmenter(data[col], labels)\n",
    "                \n",
    "                if best_split_entropy < best_overall_split_entropy:\n",
    "                    best_overall_feature = col\n",
    "                    best_overall_split_entropy = best_split_entropy\n",
    "                    best_overall_split_value = value\n",
    "                #print('best split and entropy are ')\n",
    "                #print(best_overall_split_value)\n",
    "            stop = timeit.default_timer()     \n",
    "            #print (stop - start) \n",
    "            #print('best feature split entropy: ' + str(best_overall_split_entropy))\n",
    "            #print('best feature split feature: ' + best_overall_feature)\n",
    "            #print('best feature splitting value: ' + str(best_overall_split_value))\n",
    "            #print('_____________________________')\n",
    "\n",
    "            # Join data with the label and split according to the split value\n",
    "            # Then separate out the data from the label again\n",
    "            # print('best overall feature is ' + str(best_overall_feature) + ' at level ' + str(curLevel))\n",
    "\n",
    "            data['label'] = labels\n",
    "            leftSet = data[data [best_overall_feature] <= best_overall_split_value]\n",
    "            rightSet = data[data [best_overall_feature] > best_overall_split_value]\n",
    "            \n",
    "            leftLabels = leftSet['label']\n",
    "            rightLabels = rightSet['label']\n",
    "            del leftSet['label']\n",
    "            del rightSet['label']\n",
    "            \n",
    "            #print('creating a node at level ' + str(curLevel) + ' with feature ' + best_overall_feature \n",
    "            #      + ' and split value ' + str(best_overall_split_value))\n",
    "            \n",
    "            root.feature = best_overall_feature\n",
    "            root.value = best_overall_split_value\n",
    "            root.left = Node(None, None, None, None, None)\n",
    "            self.train(root.left, leftSet, leftLabels, curLevel+1, maxLevel, percentFeatures)\n",
    "            root.right = Node(None, None, None, None, None)\n",
    "            self.train(root.right, rightSet, rightLabels, curLevel+1, maxLevel, percentFeatures)\n",
    "            \n",
    "    def predict(self, data, node):\n",
    "        #print('in predict')\n",
    "        if node.right is None and node.left is None:\n",
    "            #print('The label is ' + str(node.label))\n",
    "            #if (node.label == 0):\n",
    "            #    print(\"Therefore the label is spam\")\n",
    "            #else:\n",
    "            #    print(\"Therefore the label is ham\")\n",
    "            return node.label\n",
    "        else:\n",
    "            #print('The feature is ' + str(node.feature))\n",
    "            #print('The value is ' + str(node.value))\n",
    "            #print('The data is ' + str(data[node.feature]))\n",
    "            #print('done data')\n",
    "            if data [node.feature] <= node.value:\n",
    "                #print(str(node.feature) + ' <= ' + str(node.value) )\n",
    "                return self.predict(data, node.left)\n",
    "            else:\n",
    "                #print(str(node.feature) + ' > ' + str(node.value) )\n",
    "                return self.predict(data, node.right)\n",
    "        \n",
    "    def computeAccuracy(self, isTrain):\n",
    "        numRight = 0\n",
    "        if isTrain:\n",
    "            for index, row in self.train_data.iterrows():\n",
    "                if (self.predict(row, self.root) == self.train_labels[index]):\n",
    "                    numRight += 1\n",
    "            return numRight / len(self.train_labels)\n",
    "        \n",
    "        else:\n",
    "            for index, row in self.val_data.iterrows():\n",
    "                if (self.predict(row, self.root) == self.val_labels[index]):\n",
    "                    numRight += 1\n",
    "            \n",
    "            return numRight / len(self.val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Titanic Decision Tree</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aiswaryasankar/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc is 0.8125\n",
      "validation acc is 0.815\n",
      "training acc is 0.83875\n",
      "validation acc is 0.81\n",
      "training acc is 0.855\n",
      "validation acc is 0.795\n"
     ]
    }
   ],
   "source": [
    "# Create decision tree with all the data\n",
    "titanic_classifier = DecisionTree(titanic_train_df, titanic_train_labels,\n",
    "                                 titanic_val_df, titanic_val_labels)\n",
    "\n",
    "# Train the decision tree with different parameters for number of levels\n",
    "# After training the model find the training and validation accuracies\n",
    "\n",
    "levels = [3, 5, 7]\n",
    "\n",
    "for level in levels:\n",
    "    if ('label' in titanic_train_df.columns):\n",
    "        del titanic_train_df['label']\n",
    "    titanic_classifier.train(titanic_classifier.root, titanic_train_df, \n",
    "                             titanic_train_labels, 0, level, 1)\n",
    "    titanicTrainingAcc = titanic_classifier.computeAccuracy(True)\n",
    "    titanicValAcc = titanic_classifier.computeAccuracy(False)\n",
    "    print('training acc is ' + str(titanicTrainingAcc))\n",
    "    print('validation acc is ' + str(titanicValAcc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Titanic Submission</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aiswaryasankar/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "if ('label' in titanic_train_df.columns):\n",
    "    del titanic_train_df['label']\n",
    "    \n",
    "titanic_classifier.train(titanic_classifier.root, titanic_train_df, \n",
    "                             titanic_train_labels, 0, 3, 1)\n",
    "pred = []\n",
    "for index, row in titanic_test_df.iterrows():\n",
    "    pred.append(titanic_classifier.predict(row, titanic_classifier.root))\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "indexArr = [i for i in range(1, len(pred)+1)]\n",
    "d = OrderedDict()\n",
    "d[\"Id\"] = indexArr\n",
    "d[\"Category\"] = pred\n",
    "\n",
    "output = pd.DataFrame(data=d)\n",
    "output.to_csv('PredTitanic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex_female 0\n",
      "\tage 3.0\n",
      "\t\tsibsp 1.0\n",
      "\t\t\tfare 13.775\n",
      "\t\t\t\t0\n",
      "\t\t\t\t1\n",
      "\t\t\tpclass 3.0\n",
      "\t\t\t\t0\n",
      "\t\t\t\t0\n",
      "\t\tpclass 1.0\n",
      "\t\t\tage 54.0\n",
      "\t\t\t\t0\n",
      "\t\t\t\t0\n",
      "\t\t\tage 32.0\n",
      "\t\t\t\t0\n",
      "\t\t\t\t0\n",
      "\tpclass 2.0\n",
      "\t\tembarked_S 0\n",
      "\t\t\tpclass 1.0\n",
      "\t\t\t\t1\n",
      "\t\t\t\t1\n",
      "\t\t\tage 58.0\n",
      "\t\t\t\t1\n",
      "\t\t\t\t1\n",
      "\t\tfare 22.3583\n",
      "\t\t\tfare 7.25\n",
      "\t\t\t\t1\n",
      "\t\t\t\t1\n",
      "\t\t\tage 5.0\n",
      "\t\t\t\t0\n",
      "\t\t\t\t0\n"
     ]
    }
   ],
   "source": [
    "def printTree(node, level):\n",
    "    if node.right == None and node.left == None:\n",
    "        print('\\t' * level + str(node.label))\n",
    "    else:\n",
    "        print('\\t' * level + str(node.feature) + \" \" + str(node.value))\n",
    "        printTree(node.left, level+1)\n",
    "        printTree(node.right, level+1)\n",
    "        \n",
    "printTree(titanic_classifier.root, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Census Decision Tree</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aiswaryasankar/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc is 0.8064861148248597\n",
      "validation acc is 0.7943468296409473\n"
     ]
    }
   ],
   "source": [
    "# Create decision tree with all the data\n",
    "census_classifier = DecisionTree(census_train_df, census_train_labels,\n",
    "                                 census_val_df, census_val_labels)\n",
    "\n",
    "# Train the decision tree with different parameters for number of levels\n",
    "# After training the model find the training and validation accuracies\n",
    "\n",
    "levels = [1]\n",
    "\n",
    "for level in levels:\n",
    "    if ('label' in census_train_df.columns):\n",
    "        del census_train_df['label']\n",
    "\n",
    "    census_classifier.train(census_classifier.root, census_train_df, \n",
    "                             census_train_labels, 0, level, 1)\n",
    "    censusTrainingAcc = census_classifier.computeAccuracy(True)\n",
    "    censusValAcc = census_classifier.computeAccuracy(False)\n",
    "    print('training acc is ' + str(censusTrainingAcc))\n",
    "    print('validation acc is ' + str(censusValAcc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Census Tree Depth</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aiswaryasankar/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.839266615737204\n",
      "0.839266615737204\n",
      "0.8446142093200917\n",
      "0.8456837280366692\n",
      "0.8508785332314744\n"
     ]
    }
   ],
   "source": [
    "depths = [2, 3, 4, 5, 10, 20]\n",
    "val_acc = []\n",
    "\n",
    "for depth in depths:\n",
    "    #Botch fix\n",
    "    if ('label' in census_train_df.columns):\n",
    "        del census_train_df['label']\n",
    "        \n",
    "    census_classifier.train(census_classifier.root, census_train_df, \n",
    "                             census_train_labels, 0, depth, 1)\n",
    "    print(census_classifier.computeAccuracy(False))\n",
    "    val_acc.append(census_classifier.computeAccuracy(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11072e668>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAFyCAYAAADrieCVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1142e60f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "val_acc = [0.839266615737204, 0.839266615737204, 0.8446142093200917, 0.8456837280366692, 0.8508785332314744, 0.8343773873185638]\n",
    "depths = [2,3,4,5,10,20]\n",
    "\n",
    "plt.plot(depths, val_acc, 'ro')\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel(\"Validation accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Census Submission</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for index, row in census_test_df.iterrows():\n",
    "    pred.append(census_classifier.predict(row, census_classifier.root))\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "indexArr = [i for i in range(1, len(pred)+1)]\n",
    "d = OrderedDict()\n",
    "d[\"Id\"] = indexArr\n",
    "d[\"Category\"] = pred\n",
    "\n",
    "output = pd.DataFrame(data=d)\n",
    "output.to_csv('PredictionsCensus.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spam Decision Tree</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aiswaryasankar/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc is 0.7884605242339539\n",
      "validation acc is 0.7717781058848344\n"
     ]
    }
   ],
   "source": [
    "spam_classifier = DecisionTree(spam_train_df, spam_train_labels,\n",
    "                                 spam_val_df, spam_val_labels)\n",
    "\n",
    "# Train the decision tree with different parameters for number of levels\n",
    "# After training the model find the training and validation accuracies\n",
    "\n",
    "levels = [10]\n",
    "\n",
    "for level in levels:\n",
    "    if ('label' in spam_train_df.columns):\n",
    "        del spam_train_df['label']\n",
    "        \n",
    "    spam_classifier.train(spam_classifier.root, spam_train_df, \n",
    "                             spam_train_labels, 0, level, 1)\n",
    "    spamTrainingAcc = spam_classifier.computeAccuracy(True)\n",
    "    spamValAcc = spam_classifier.computeAccuracy(False)\n",
    "    print('training acc is ' + str(spamTrainingAcc))\n",
    "    print('validation acc is ' + str(spamValAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spam Prediction Splits</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in predict\n",
      "28 > 0.0\n",
      "in predict\n",
      "13 <= 0.0\n",
      "in predict\n",
      "3 <= 0.0\n",
      "in predict\n",
      "29 > 0.0\n",
      "in predict\n",
      "28 <= 2.0\n",
      "in predict\n",
      "15 <= 1.0\n",
      "in predict\n",
      "9 <= 0.0\n",
      "in predict\n",
      "31 <= 0.0\n",
      "in predict\n",
      "26 <= 0.0\n",
      "in predict\n",
      "29 > 3.0\n",
      "in predict\n",
      "27 <= 0.0\n",
      "in predict\n",
      "Therefore the label is spam\n",
      "in predict\n",
      "28 > 0.0\n",
      "in predict\n",
      "13 <= 0.0\n",
      "in predict\n",
      "3 <= 0.0\n",
      "in predict\n",
      "29 <= 0.0\n",
      "in predict\n",
      "28 <= 29.0\n",
      "in predict\n",
      "6 <= 0.0\n",
      "in predict\n",
      "26 <= 0.0\n",
      "in predict\n",
      "0 <= 0.0\n",
      "in predict\n",
      "28 > 1.0\n",
      "in predict\n",
      "16 <= 0.0\n",
      "in predict\n",
      "25 > 1.0\n",
      "in predict\n",
      "Therefore the label is ham\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a data point from spam and ham\n",
    "#for i in range(20):\n",
    "#    if spam_classifier.predict(spam_train_df.iloc[i], spam_classifier.root) == 'ham':\n",
    "#        print(i)\n",
    "sample1 = spam_train_df.iloc[9]\n",
    "sample2 = spam_train_df.iloc[4]\n",
    "\n",
    "spam_classifier.predict(sample1, spam_classifier.root)\n",
    "spam_classifier.predict(sample2, spam_classifier.root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spam Submission</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for index, row in spam_test_df.iterrows():\n",
    "    pred.append(spam_classifier.predict(row, spam_classifier.root))\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "indexArr = [i for i in range(len(pred))]\n",
    "d = OrderedDict()\n",
    "d[\"Id\"] = indexArr\n",
    "d[\"Category\"] = pred\n",
    "\n",
    "output = pd.DataFrame(data=d)\n",
    "output.to_csv('PredictionsSpam.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Random Forest</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \n",
    "    def __init__(self, train_df, train_labels, val_df, val_labels):\n",
    "        self.train_df = train_df\n",
    "        self.train_labels = train_labels\n",
    "        self.val_df = val_df\n",
    "        self.val_labels = val_labels\n",
    "        self.forest = []\n",
    "        \n",
    "    def train(self, data, labels, numTrees, featuresPerSplit, dataSubsample):\n",
    "        \n",
    "        for i in range(numTrees):\n",
    "            # Create the data subset to use for this tree\n",
    "            # First I need to combine the data and the labels in order to make sure that I \n",
    "            # subsample both of the datasets at the same time and they match up appropriately\n",
    "            \n",
    "            merged = pd.concat([data, labels], axis=1)\n",
    "            \n",
    "            # Now I need to subsample a number of the rows\n",
    "            # This is another hyperparameter\n",
    "            merged.sample(frac=dataSubsample, replace=True)\n",
    "            \n",
    "            # Now split this back up into the train and labels dataframes\n",
    "            data = merged.iloc[:, :-1]\n",
    "            labels = merged.iloc[:, -1]\n",
    "            \n",
    "            # This will create a list of classifiers\n",
    "            # Now I need to actually go in and train each of them\n",
    "            tree = DecisionTree(data, labels, self.val_df, self.val_labels)\n",
    "            \n",
    "            # Botch fix again\n",
    "            if ('label' in data.columns):\n",
    "                del data['label']\n",
    "                print('deleted label')\n",
    "                \n",
    "            tree.train(tree.root, data, labels, 0, 3, featuresPerSplit)\n",
    "            self.forest.append(tree)\n",
    "            \n",
    "    def computeForestAcc(self, isTrain):\n",
    "        # In order to predict I need to pass each point through every tree and take the most\n",
    "        # common output\n",
    "        numRight = 0\n",
    "        \n",
    "        if isTrain:\n",
    "            data = self.train_df\n",
    "            labels = self.train_labels\n",
    "        else:\n",
    "            data = self.val_df\n",
    "            labels = self.val_labels\n",
    "            \n",
    "        # Computes the predictions with each of the trees in the forest\n",
    "        for index, row in data.iterrows():\n",
    "            treePred = []\n",
    "            for tree in self.forest:\n",
    "                treePred.append(tree.predict(row, tree.root))\n",
    "                \n",
    "            # Choose the most common prediction out of all tree predictions\n",
    "            bestPred = np.argmax([treePred.count(0.0), treePred.count(1.0)])\n",
    "            if (bestPred == labels[index]):\n",
    "                numRight += 1\n",
    "                \n",
    "        return numRight / len(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run Random Forest</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_rf(train_df, train_labels, val_df, val_labels):\n",
    "    forest = RandomForest(train_df, train_labels, val_df, val_labels)\n",
    "\n",
    "    numTrees = [5, 10, 15]\n",
    "    featuresPerSplits = [.5]\n",
    "    dataSubsamples = [.5]\n",
    "\n",
    "    for numTree in numTrees:\n",
    "        for featuresPerSplit in featuresPerSplits:\n",
    "            for dataSubsample in dataSubsamples:\n",
    "                \n",
    "                forest.train(train_df, train_labels, numTree, featuresPerSplit, dataSubsample)\n",
    "                trainingAcc = forest.computeForestAcc(True)\n",
    "                valAcc = forest.computeForestAcc(False)\n",
    "            \n",
    "                print('training acc is ' + str(trainingAcc))\n",
    "                print('validation acc is ' + str(valAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Titanic Random Forest</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aiswaryasankar/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "training acc is 0.81625\n",
      "validation acc is 0.8\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "training acc is 0.8175\n",
      "validation acc is 0.815\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "training acc is 0.815\n",
      "validation acc is 0.81\n"
     ]
    }
   ],
   "source": [
    "run_rf(titanic_train_df, titanic_train_labels, titanic_val_df, titanic_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create random forest\n",
    "\n",
    "titanic_forest = RandomForest(titanic_train_df, titanic_train_labels,\n",
    "                                 titanic_val_df, titanic_val_labels)\n",
    "\n",
    "# Train the decision tree with different parameters for number of levels\n",
    "# After training the model find the training and validation accuracies\n",
    "# There are 3 hyperparameters: number of trees, percent of features per split and \n",
    "# fraction of data used for bagging\n",
    "# Percent features per split is passed into DecisionTree and dataSubsample is used in \n",
    "# the RandomForest train method to select data to be passed in\n",
    "# Thus I need three hyperparameter loops\n",
    "\n",
    "numTrees = [5, 10, 15]\n",
    "featuresPerSplits = [.5]\n",
    "dataSubsamples = [.5]\n",
    "\n",
    "for numTree in numTrees:\n",
    "    for featuresPerSplit in featuresPerSplits:\n",
    "        for dataSubsample in dataSubsamples:\n",
    "                \n",
    "            titanic_forest.train(titanic_train_df, titanic_train_labels, \n",
    "                     numTree, featuresPerSplit, dataSubsample)\n",
    "            titanicTrainingAcc = titanic_forest.computeForestAcc(True)\n",
    "            titanicValAcc = titanic_forest.computeForestAcc(False)\n",
    "            \n",
    "            print('training acc is ' + str(titanicTrainingAcc))\n",
    "            print('validation acc is ' + str(titanicValAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Census Random Forest</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aiswaryasankar/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "training acc is 0.8435005156805073\n",
      "validation acc is 0.8372803666921314\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "training acc is 0.8446464723633447\n",
      "validation acc is 0.8400305576776165\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "training acc is 0.8450284579242905\n",
      "validation acc is 0.8394194041252865\n"
     ]
    }
   ],
   "source": [
    "# Create random forest\n",
    "\n",
    "census_forest = RandomForest(census_train_df, census_train_labels,\n",
    "                                 census_val_df, census_val_labels)\n",
    "\n",
    "# Train the decision tree with different parameters for number of levels\n",
    "# After training the model find the training and validation accuracies\n",
    "# There are 3 hyperparameters: number of trees, percent of features per split and \n",
    "# fraction of data used for bagging\n",
    "# Percent features per split is passed into DecisionTree and dataSubsample is used in \n",
    "# the RandomForest train method to select data to be passed in\n",
    "# Thus I need three hyperparameter loops\n",
    "\n",
    "numTrees = [5, 10, 15]\n",
    "featuresPerSplits = [.5]\n",
    "dataSubsamples = [.5]\n",
    "\n",
    "for numTree in numTrees:\n",
    "    for featuresPerSplit in featuresPerSplits:\n",
    "        for dataSubsample in dataSubsamples:\n",
    "                \n",
    "            census_forest.train(census_train_df, census_train_labels, \n",
    "                     numTree, featuresPerSplit, dataSubsample)\n",
    "            censusTrainingAcc = census_forest.computeForestAcc(True)\n",
    "            censusValAcc = census_forest.computeForestAcc(False)\n",
    "            \n",
    "            print('training acc is ' + str(censusTrainingAcc))\n",
    "            print('validation acc is ' + str(censusValAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spam Random Forest</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aiswaryasankar/anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "deleted label\n",
      "training acc is 0.7318706819260588\n",
      "validation acc is 0.7304366167475216\n"
     ]
    }
   ],
   "source": [
    "# Create random forest\n",
    "\n",
    "spam_forest = RandomForest(spam_train_df, spam_train_labels,\n",
    "                                 spam_val_df, spam_val_labels)\n",
    "\n",
    "numTrees = [15]\n",
    "featuresPerSplits = [.5]\n",
    "dataSubsamples = [.5]\n",
    "\n",
    "for numTree in numTrees:\n",
    "    for featuresPerSplit in featuresPerSplits:\n",
    "        for dataSubsample in dataSubsamples:\n",
    "                \n",
    "            spam_forest.train(spam_train_df, spam_train_labels, \n",
    "                     numTree, featuresPerSplit, dataSubsample)\n",
    "            spamTrainingAcc = spam_forest.computeForestAcc(True)\n",
    "            spamValAcc = spam_forest.computeForestAcc(False)\n",
    "            \n",
    "            print('training acc is ' + str(spamTrainingAcc))\n",
    "            print('validation acc is ' + str(spamValAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest Classifications</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3, 0.0], 4], [[28, 0.0], 11]]\n",
      "[[3, 0.0], [3, 0.0], [3, 0.0], [3, 0.0], [28, 0.0], [28, 0.0], [28, 0.0], [28, 0.0], [28, 0.0], [28, 0.0], [28, 0.0], [28, 0.0], [28, 0.0], [28, 0.0], [28, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "# Find and state the most common splits made at the root node of the trees\n",
    "rootLabels = []\n",
    "for tree in spam_forest.forest:\n",
    "    rootLabels.append([tree.root.feature, tree.root.value])\n",
    "    \n",
    "from itertools import groupby\n",
    "rootLabels.sort()\n",
    "numTrees = len(rootLabels)\n",
    "print([[key, len(list(group))] for key, group in groupby(rootLabels)])\n",
    "print(rootLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
